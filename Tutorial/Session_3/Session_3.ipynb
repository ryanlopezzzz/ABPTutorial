{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3\n",
    "\n",
    "## Implementing a 2D simulation of Active Brownian Particles (ABP) in GPUs\n",
    "\n",
    "In the second session of this tutorial we port the C++ and the Python code developed in Session 1 and 2 to GPUs. We keep the structure and naming the same as in the c++ version and only make changes where necessary. We are going to focus on how to implement the **force** and **integrator** algorithms in parallel keeping all the rest of the code as before.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./layout_interface.png\" style=\"width: 600px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel computing: a short introduction\n",
    "\n",
    "From a practical point of view parallel computing can be defined as a form of computation in which many calculations are executed simultaneously. Large problems can often be divided into smaller ones, which are then solved concurrently on different computing resources. \n",
    "\n",
    "# Sequential vs Parallel\n",
    "\n",
    "As we've seen in the previous tutorials a computer program consist in a series of calculations that performs a specified task. For example, our ``evolve`` step in the ``evolver`` class needs to perform **sequentially** the following calculations:\n",
    "1. *Check is neighbour list needs rebuilding*\n",
    "2. *Perform the preintegration step, i.e., step before forces and torques are computed*\n",
    "3. *Apply period boundary conditions*\n",
    "4. *Reset all forces and toques*\n",
    "5. *Compute all forces and torques*\n",
    "6. *Perform the second step of integration*\n",
    "7. *Apply period boundary conditions*\n",
    "\n",
    "In general, one can classify the relationship between two pieces of computation as the ones that are: **(a)**  related by a precedence restraint and therefore must be calculated sequentially, like the ``evolve`` step, and **(b)** are not  related by a precedence restraints and therefore can be calculated concurrently. Any program containing tasks that are performed concurrently is a **parallel program**. \n",
    "\n",
    "## Parallelism\n",
    "\n",
    "There are two fundamental types of parallelism:\n",
    "\n",
    "* Data parallelism: when there are many data items that can be operated *on* at the same time.\n",
    "* Task parallelism: when there are many tasks that can be operated independently and largely in parallel.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./parellel_comp.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "<sub><sup>(Left) Data parallelism focuses on distributing the data across multiple cores. (Right) Task parallelism focuses on distributing functions across multiple cores.</sup></sub>\n",
    "</div>\n",
    "\n",
    "\n",
    "In the session we are going to focus on graphics processing unit or GPUs which are especially well-suited to address problems that can be expressed as data-parallel computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs computing: Heterogeneous Architecture\n",
    "\n",
    "Nowadays a typical compute node consists of a multicore central processing unit (**CPU**) sockets and one or more many-core GPUs. The GPU acts a co-processor to a CPU operating through a **PCI-Express bus**. GPUs were designed to fulfill specific needs, especially for workloads in the graphics industry like video games and rendering.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./GPU-transistor2.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "In terms of specificity a CPU core is designed to optimize the execution of sequential programs. In contrast, a GPU core is optimized for data-parallel tasks focusing on the throughput of parallel programs. Since the GPUs function as a \"co-processor\", the CPU is called the ``host`` and the GPU is called the ``device``.\n",
    "\n",
    "In any GPU based application consist of two types of parts:\n",
    "\n",
    "* Host code\n",
    "* Device code\n",
    "\n",
    "As the name suggest the *host code* run on CPUs and *device code* runs on GPUs. Typically the CPU is in charge of initialize the environment for the GPU, i.e, allocate data, copy the data, etc. \n",
    "\n",
    "## The CUDA plattaform \n",
    "\n",
    "**CUDA** is a parallel computing platform and programming model that makes use of the parallel compute engine in NVIDIA GPUs. From the practical point if view, CUDA language interface are the familiar C, C++, Fortran, openCL programming languages. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./cuda-plataform.png\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "### The CUDA Programming Model\n",
    "\n",
    "Here we are going to learn how to write CUDA-C/C++ code to make use of the GPU. As before our application consist in two \"types\" of code. The first one is the ``host`` code and the second one is the ``device`` code. The device code (which is exclusively executed in the GPU) is denominated **``kernel``**. The kernels function are identified by the keyword ``__global__``, and by the fact that **never** return any value (``void`` void return type). The following example show the first ``hello_world`` program:\n",
    "\n",
    "```c\n",
    "//@file: hello_world.cu\n",
    "#include \"hello_world.hpp\"\n",
    "\n",
    "// device code start\n",
    "__global__ void hello_world_kernel()\n",
    "{\n",
    "    printf(\"hello\");\n",
    "}\n",
    "// device code end\n",
    "\n",
    "// host code start\n",
    "void call_hello_world_kernel(void)\n",
    "{\n",
    "    hello_kernel<<<1,1>>>();\n",
    "}\n",
    "// host code end\n",
    "\n",
    "//@file: hello_world.hpp\n",
    "void call_hello_world_kernel(void); //forward declaration\n",
    "\n",
    "//@file: main.cpp\n",
    "#include \"hello_world.hpp\"\n",
    "\n",
    "// host code start\n",
    "int main()\n",
    "{\n",
    "    ///...\n",
    "\n",
    "    call_hello_world_kernel();\n",
    "    \n",
    "    ///...\n",
    "}\n",
    "// host code end\n",
    "\n",
    "```\n",
    "Kernels codes have several properties: \n",
    "1. are identified by the ``__global__`` qualifier with ``void`` return type\n",
    "2. are only executable on the device\n",
    "3. are only callable by the host\n",
    "\n",
    "#### Where is the parallelism here?\n",
    "\n",
    "So far we haven't seen any parallelism. To understand \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [https://devblogs.nvidia.com/](https://devblogs.nvidia.com/)\n",
    "* Cheng, John, Max Grossman, and Ty McKercher. Professional CUDA c programming. John Wiley & Sons, 2014.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
